{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multivariate Normal Dueling Bandit Demo\n",
        "\n",
        "This notebook demonstrates how to tune the batch size for the multivariate normal dueling bandit using **SciPy's D'Agostino–Pearson K² test** instead of the custom Mardia test used previously.  It also explains each step in the process and shows how to run the bandit algorithm once the batch size is selected.\n",
        "\n",
        "The general steps are:\n",
        "\n",
        "1. Load the cost curves, candidate levee heights, posterior parameter draws and mean sea level (MSL) paths.\n",
        "2. Tune the batch size ``S`` for the central limit theorem by sampling a number of mean cost vectors and applying SciPy's normality test on each coordinate.  We start from a small ``S`` and double it until all p–values exceed our chosen significance threshold.\n",
        "3. Use the tuned batch size to run the dueling bandit algorithm implemented in ``mvn_dueling_bandit.py``.\n",
        "4. Inspect the posterior probability that each height is optimal and visualise the convergence over time.\n",
        "\n",
        "By running this notebook, you can gain confidence that the sample mean cost vectors are approximately multivariate normal and that the bandit algorithm uses a statistically justified stopping rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "# Import functions from your project\n",
        "from optimal_levee_bandit import load_cost_curves, prune_candidates\n",
        "from mvn_dueling_bandit import _simulate_single_scenario, run_mvn_dueling_bandit\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load cost curves and parameters\n",
        "\n",
        "Here we load the damage and protection cost curves for the selected city, define the analysis horizon, and load posterior parameter draws and mean sea level paths.  These objects are required for the Monte Carlo simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 25 potential levee heights.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Specify input files\n",
        "damage_file     = \"Damage_cost_curves.tab\"\n",
        "protection_file = \"Protection_cost_curves_high_estimate.tab\"\n",
        "city = \"Halmstad\"\n",
        "\n",
        "# Load cost curves\n",
        "heights, damage_costs, protection_costs = load_cost_curves(damage_file, protection_file, city)\n",
        "\n",
        "# Define analysis horizon (inclusive)\n",
        "years_range = (2025, 2100)\n",
        "\n",
        "# Use all heights as candidates (no pruning)\n",
        "candidate_indices = np.arange(heights.size, dtype=int)\n",
        "\n",
        "print(f\"Loaded {len(heights)} potential levee heights.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSL paths shape restricted to horizon: (20000, 76)\n"
          ]
        }
      ],
      "source": [
        "# Load posterior parameter draws and mean sea level paths\n",
        "# Adjust the filename to match your actual .npz file\n",
        "pp = np.load(\"pp_inputs_halmsdad_pp_mixture_2025_2100.npz\")\n",
        "posterior_params = {\n",
        "    \"eta0\": pp[\"eta0\"],\n",
        "    \"eta1\": pp[\"eta1\"],\n",
        "    \"alpha0\": pp[\"alpha0\"],\n",
        "    \"xi\": pp[\"xi\"],\n",
        "    \"u\": float(pp[\"u\"]),  # threshold in cm\n",
        "}\n",
        "\n",
        "# Extract mean sea level paths and corresponding years\n",
        "years_future   = pp[\"years_future\"]\n",
        "X_future_paths = pp[\"X_future_paths\"]  # shape (M_pred, T_future) in cm\n",
        "\n",
        "# Restrict MSL paths to the analysis horizon\n",
        "start_year, end_year = years_range\n",
        "mask = (years_future >= start_year) & (years_future <= end_year)\n",
        "years_slice = years_future[mask]\n",
        "X_paths_slice_cm = X_future_paths[:, mask]\n",
        "\n",
        "print(f\"MSL paths shape restricted to horizon: {X_paths_slice_cm.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tuning the batch size using SciPy's normality test\n",
        "\n",
        "To apply the multivariate central limit theorem, we want the vector of mean total costs across the candidate heights to be approximately multivariate normal.  One practical way to check this is to apply a **univariate normality test** to each coordinate of the mean vectors.  If each marginal distribution looks normal (as measured by p–values exceeding a significance threshold), the joint distribution is often well approximated by a multivariate normal for large ``S``.\n",
        "\n",
        "We use the [D'Agostino–Pearson K² test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html) implemented in `scipy.stats.normaltest`.  For a given batch size ``S``, we sample many cost vectors, average them, and compute the p–value of the omnibus normality test for each coordinate.  If **all** p–values exceed our chosen ``significance`` threshold (e.g., 0.05), we accept this ``S``.  Otherwise we double ``S`` and try again.  This tuning process repeats until acceptance or until a maximum ``S`` is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Example usage: tune S with verbose output\u001b[39;00m\n\u001b[32m     50\u001b[39m rng = np.random.default_rng(\u001b[32m12345\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m tuned_S, mu_samples = \u001b[43mtune_batch_size_manual\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_S\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignificance\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_replicates\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTuned batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtuned_S\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtune_batch_size_manual\u001b[39m\u001b[34m(initial_S, significance, test_replicates, max_S, rng)\u001b[39m\n\u001b[32m     30\u001b[39m S = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(initial_S))\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     mu_samples = \u001b[43mcompute_mu_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_replicates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# normaltest returns (statistic, p-value) for each column\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcompute_mu_samples\u001b[39m\u001b[34m(S, rng, test_replicates)\u001b[39m\n\u001b[32m      7\u001b[39m     costs_cum = np.empty((num_cands), dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(S):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         costs_cum= costs_cum+\u001b[43m_simulate_single_scenario\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcand_heights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcand_protections\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprotection_costs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheight_grid_m\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdamage_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdamage_costs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX_paths_slice_cm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_paths_slice_cm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mposterior_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposterior_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43mu_cm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mposterior_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m60.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     mu_samples[r] = costs_cum/S\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mu_samples\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\OneDrive\\Skrivbord\\Python_vscode\\flood_bandit\\mvn_dueling_bandit.py:198\u001b[39m, in \u001b[36m_simulate_single_scenario\u001b[39m\u001b[34m(rng, cand_heights, cand_protections, height_grid_m, damage_grid, X_paths_slice_cm, posterior_params, u_cm)\u001b[39m\n\u001b[32m    196\u001b[39m X_t_series_cm = X_paths_slice_cm[m_path]\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# Simulate annual maxima\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m maxima_cm = \u001b[43msimulate_annual_max_pp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43meta0\u001b[49m\u001b[43m=\u001b[49m\u001b[43meta0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43meta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43meta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha0\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_t_series_cm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_t_series_cm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mu_cm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mu_cm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m maxima_m = maxima_cm * \u001b[32m0.01\u001b[39m  \u001b[38;5;66;03m# convert cm -> m\u001b[39;00m\n\u001b[32m    208\u001b[39m total_costs = np.empty(num_cands, dtype=\u001b[38;5;28mfloat\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\OneDrive\\Skrivbord\\Python_vscode\\flood_bandit\\mvn_dueling_bandit.py:118\u001b[39m, in \u001b[36msimulate_annual_max_pp\u001b[39m\u001b[34m(eta0, eta1, alpha0, xi, X_t_series_cm, u_cm, rng)\u001b[39m\n\u001b[32m    116\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    117\u001b[39m             Z = (sigma / xi) * ((\u001b[32m1.0\u001b[39m - U) ** (-xi) - \u001b[32m1.0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m         maxima_cm[j] = u_cm + \u001b[38;5;28mfloat\u001b[39m(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m maxima_cm\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\johan\\miniconda3\\envs\\flood_pymc\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3047\u001b[39m, in \u001b[36m_max_dispatcher\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3043\u001b[39m         kwargs[\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m] = keepdims\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _methods._ptp(a, axis=axis, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_max_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m, initial=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3048\u001b[39m                     where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   3049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   3052\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[32m   3053\u001b[39m \u001b[38;5;129m@set_module\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3054\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, initial=np._NoValue,\n\u001b[32m   3055\u001b[39m          where=np._NoValue):\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def compute_mu_samples(S, rng, test_replicates=200):\n",
        "    \"\"\"Compute test_replicates mean cost vectors using batch size S.\"\"\"\n",
        "    num_cands = candidate_indices.size\n",
        "    mu_samples = np.empty((test_replicates, num_cands), dtype=float)\n",
        "    for r in range(test_replicates):\n",
        "        # Collect costs for S independent scenarios\n",
        "        costs_cum = np.empty(num_cands, dtype=float)\n",
        "        for s in range(S):\n",
        "            costs_cum= costs_cum+_simulate_single_scenario(\n",
        "                rng,\n",
        "                cand_heights=heights[candidate_indices],\n",
        "                cand_protections=protection_costs[candidate_indices],\n",
        "                height_grid_m=heights,\n",
        "                damage_grid=damage_costs,\n",
        "                X_paths_slice_cm=X_paths_slice_cm,\n",
        "                posterior_params=posterior_params,\n",
        "                u_cm=float(posterior_params.get(\"u\", 60.0)),\n",
        "            )\n",
        "        mu_samples[r] = costs_cum/S\n",
        "    return mu_samples\n",
        "\n",
        "def tune_batch_size_manual(initial_S=100, significance=0.05, test_replicates=200, max_S=1_000_000, rng=None):\n",
        "    \"\"\"\n",
        "    Manually tune S by checking univariate normality with SciPy.\n",
        "\n",
        "    Returns the tuned batch size and the corresponding mu_samples.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    S = max(1, int(initial_S))\n",
        "    while True:\n",
        "        mu_samples = compute_mu_samples(S, rng, test_replicates)\n",
        "        # normaltest returns (statistic, p-value) for each column\n",
        "        try:\n",
        "            stat, pvals = normaltest(mu_samples, axis=0)\n",
        "        except Exception:\n",
        "            # If the test fails, set p-values to zero to force doubling S\n",
        "            pvals = np.zeros(candidate_indices.size)\n",
        "            print(f\"Tested S={S}: normality test failed, doubling S.\")\n",
        "        min_p = float(np.min(pvals))\n",
        "        print(f\"Tested S={S}: minimum p-value = {min_p:.4f}\")\n",
        "        if np.all(pvals > significance):\n",
        "            return S, mu_samples\n",
        "        S *= 1.4\n",
        "        S = max(1, int(S))\n",
        "        if S > max_S:\n",
        "            raise RuntimeError(\"Failed to achieve approximate normality before reaching max_S.\")\n",
        "\n",
        "# Example usage: tune S with verbose output\n",
        "rng = np.random.default_rng(12345)\n",
        "tuned_S, mu_samples = tune_batch_size_manual(initial_S=70, significance=0.05, test_replicates=200, rng=rng)\n",
        "print(f\"Tuned batch size: {tuned_S}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code cell above, ``tune_batch_size_manual`` repeatedly samples ``test_replicates`` mean cost vectors, applies SciPy's **normaltest** along each dimension, prints the minimum p–value, and doubles ``S`` until all p–values exceed the significance threshold.  Once the condition is met, it returns the tuned batch size and the corresponding sample means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run the multivariate normal dueling bandit\n",
        "\n",
        "Now that we have a tuned batch size, we can run the dueling bandit.  The function `run_mvn_dueling_bandit` encapsulates the entire adaptive procedure: it reuses the normality tuning procedure internally (so we set ``initial_S=tuned_S`` to avoid starting from scratch), computes a multivariate normal prior, and updates the posterior as new floods are simulated.  The algorithm stops when the posterior probability that some height is optimal exceeds ``1 − delta``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "delta = 0.05  # target misselection probability\n",
        "rng = np.random.default_rng(98765)\n",
        "\n",
        "# Run the bandit algorithm with the tuned initial_S\n",
        "best_height, history = run_mvn_dueling_bandit(\n",
        "    heights=heights,\n",
        "    damage_costs=damage_costs,\n",
        "    protection_costs=protection_costs,\n",
        "    candidate_indices=candidate_indices,\n",
        "    years_all=years_future,\n",
        "    X_pred_paths_cm=X_future_paths,\n",
        "    posterior_params=posterior_params,\n",
        "    years_range=years_range,\n",
        "    delta=delta,\n",
        "    initial_S=tuned_S,    # use the manually tuned S as a starting point\n",
        "    significance=0.05,\n",
        "    test_replicates=200,\n",
        "    post_samples=2000,\n",
        "    max_rounds=50_000,\n",
        "    check_every=1_000,\n",
        "    rng=rng,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(f\"Selected best design height: {best_height:.2f} m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Inspect the convergence history\n",
        "\n",
        "The `history` dictionary returned by `run_mvn_dueling_bandit` contains the rounds at which the stopping rule was evaluated, the index and value of the current best height, and the posterior probability `p_best` that this candidate is optimal.  We can convert this to a Pandas DataFrame and plot the probability of the best height and the evolution of the selected height over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Build DataFrame\n",
        "hist_df = pd.DataFrame(history)\n",
        "display(hist_df.tail())\n",
        "\n",
        "# Plot posterior probability of the best height\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.plot(hist_df['rounds'], hist_df['p_best'], marker='o')\n",
        "ax1.axhline(1 - delta, color='red', linestyle='--', label=f'1 - delta = {1 - delta}')\n",
        "ax1.set_xlabel('Rounds')\n",
        "ax1.set_ylabel('Posterior prob. best')\n",
        "ax1.set_title('Convergence of the posterior probability for the best height')\n",
        "ax1.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot evolution of the empirical best height\n",
        "fig, ax2 = plt.subplots()\n",
        "ax2.step(hist_df['rounds'], hist_df['best_height'], where='post', marker='o')\n",
        "ax2.set_xlabel('Rounds')\n",
        "ax2.set_ylabel('Height (m)')\n",
        "ax2.set_title('Evolution of the empirically best height')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flood_pymc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
