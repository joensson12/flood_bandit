{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multivariate Normal Dueling Bandit Demo\n",
        "\n",
        "This notebook demonstrates how to tune the batch size for the multivariate normal dueling bandit using **SciPy's D'Agostino–Pearson K² test** instead of the custom Mardia test used previously.  It also explains each step in the process and shows how to run the bandit algorithm once the batch size is selected.\n",
        "\n",
        "The general steps are:\n",
        "\n",
        "1. Load the cost curves, candidate levee heights, posterior parameter draws and mean sea level (MSL) paths.\n",
        "2. Tune the batch size ``S`` for the central limit theorem by sampling a number of mean cost vectors and applying SciPy's normality test on each coordinate.  We start from a small ``S`` and double it until all p–values exceed our chosen significance threshold.\n",
        "3. Use the tuned batch size to run the dueling bandit algorithm implemented in ``mvn_dueling_bandit.py``.\n",
        "4. Inspect the posterior probability that each height is optimal and visualise the convergence over time.\n",
        "\n",
        "By running this notebook, you can gain confidence that the sample mean cost vectors are approximately multivariate normal and that the bandit algorithm uses a statistically justified stopping rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "# Import functions from your project\n",
        "from optimal_levee_bandit import load_cost_curves, prune_candidates\n",
        "from mvn_dueling_bandit import _simulate_single_scenario, run_mvn_dueling_bandit\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load cost curves and parameters\n",
        "\n",
        "Here we load the damage and protection cost curves for the selected city, define the analysis horizon, and load posterior parameter draws and mean sea level paths.  These objects are required for the Monte Carlo simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 25 potential levee heights.\n"
          ]
        }
      ],
      "source": [
        "from optimal_levee_bandit_gpu import load_cost_curves\n",
        "from mvn_dueling_bandit import _simulate_single_scenario\n",
        "from scipy.stats import normaltest\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Specify input files\n",
        "damage_file     = \"data/Damage_cost_curves.tab\"\n",
        "protection_file = \"data/Protection_cost_curves_high_estimate.tab\"\n",
        "city = \"Halmstad\"\n",
        "\n",
        "# Load cost curves\n",
        "heights, damage_costs, protection_costs = load_cost_curves(damage_file, protection_file, city)\n",
        "\n",
        "# Define analysis horizon (inclusive)\n",
        "years_range = (2025, 2100)\n",
        "\n",
        "# Use all heights as candidates (no pruning)\n",
        "candidate_indices = np.arange(heights.size, dtype=int)\n",
        "\n",
        "print(f\"Loaded {len(heights)} potential levee heights.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSL paths shape restricted to horizon: (20000, 76)\n"
          ]
        }
      ],
      "source": [
        "# Load posterior parameter draws and mean sea level paths\n",
        "# Adjust the filename to match your actual .npz file\n",
        "pp = np.load(\"data/pp_inputs_halmsdad_pp_mixture_2025_2100.npz\")\n",
        "posterior_params = {\n",
        "    \"eta0\": pp[\"eta0\"],\n",
        "    \"eta1\": pp[\"eta1\"],\n",
        "    \"alpha0\": pp[\"alpha0\"],\n",
        "    \"xi\": pp[\"xi\"],\n",
        "    \"u\": float(pp[\"u\"]),  # threshold in cm\n",
        "}\n",
        "\n",
        "# Extract mean sea level paths and corresponding years\n",
        "years_future   = pp[\"years_future\"]\n",
        "X_future_paths = pp[\"X_future_paths\"]  # shape (M_pred, T_future) in cm\n",
        "\n",
        "# Restrict MSL paths to the analysis horizon\n",
        "start_year, end_year = years_range\n",
        "mask = (years_future >= start_year) & (years_future <= end_year)\n",
        "years_slice = years_future[mask]\n",
        "X_paths_slice_cm = X_future_paths[:, mask]\n",
        "\n",
        "print(f\"MSL paths shape restricted to horizon: {X_paths_slice_cm.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tuning the batch size using SciPy's normality test\n",
        "\n",
        "To apply the multivariate central limit theorem, we want the vector of mean total costs across the candidate heights to be approximately multivariate normal.  One practical way to check this is to apply a **univariate normality test** to each coordinate of the mean vectors.  If each marginal distribution looks normal (as measured by p–values exceeding a significance threshold), the joint distribution is often well approximated by a multivariate normal for large ``S``.\n",
        "\n",
        "We use the [D'Agostino–Pearson K² test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html) implemented in `scipy.stats.normaltest`.  For a given batch size ``S``, we sample many cost vectors, average them, and compute the p–value of the omnibus normality test for each coordinate.  If **all** p–values exceed our chosen ``significance`` threshold (e.g., 0.05), we accept this ``S``.  Otherwise we double ``S`` and try again.  This tuning process repeats until acceptance or until a maximum ``S`` is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tested S=70: minimum p-value = 0.0000\n"
          ]
        }
      ],
      "source": [
        "def compute_mu_samples(S, rng, test_replicates=200):\n",
        "    \"\"\"Compute test_replicates mean cost vectors using batch size S.\"\"\"\n",
        "    num_cands = candidate_indices.size\n",
        "    mu_samples = np.empty((test_replicates, num_cands), dtype=float)\n",
        "    for r in range(test_replicates):\n",
        "        # Collect costs for S independent scenarios\n",
        "        costs_cum = np.empty(num_cands, dtype=float)\n",
        "        for s in range(S):\n",
        "            costs_cum= costs_cum+_simulate_single_scenario(\n",
        "                rng,\n",
        "                cand_heights=heights[candidate_indices],\n",
        "                cand_protections=protection_costs[candidate_indices],\n",
        "                height_grid_m=heights,\n",
        "                damage_grid=damage_costs,\n",
        "                X_paths_slice_cm=X_paths_slice_cm,\n",
        "                posterior_params=posterior_params,\n",
        "                u_cm=float(posterior_params.get(\"u\", 60.0)),\n",
        "            )\n",
        "        mu_samples[r] = costs_cum/S\n",
        "    return mu_samples\n",
        "\n",
        "def tune_batch_size_manual(initial_S=100, significance=0.05, test_replicates=200, max_S=1_000_000, rng=None):\n",
        "    \"\"\"\n",
        "    Manually tune S by checking univariate normality with SciPy.\n",
        "\n",
        "    Returns the tuned batch size and the corresponding mu_samples.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    S = max(1, int(initial_S))\n",
        "    while True:\n",
        "        mu_samples = compute_mu_samples(S, rng, test_replicates)\n",
        "        # normaltest returns (statistic, p-value) for each column\n",
        "        try:\n",
        "            stat, pvals = normaltest(mu_samples, axis=0)\n",
        "        except Exception:\n",
        "            # If the test fails, set p-values to zero to force doubling S\n",
        "            pvals = np.zeros(candidate_indices.size)\n",
        "            print(f\"Tested S={S}: normality test failed, doubling S.\")\n",
        "        min_p = float(np.min(pvals))\n",
        "        print(f\"Tested S={S}: minimum p-value = {min_p:.4f}\")\n",
        "        if np.all(pvals > significance):\n",
        "            return S, mu_samples\n",
        "        S *= 1.4\n",
        "        S = max(1, int(S))\n",
        "        if S > max_S:\n",
        "            raise RuntimeError(\"Failed to achieve approximate normality before reaching max_S.\")\n",
        "\n",
        "# Example usage: tune S with verbose output\n",
        "rng = np.random.default_rng(12345)\n",
        "tuned_S, mu_samples = tune_batch_size_manual(initial_S=70, significance=0.05, test_replicates=200, rng=rng)\n",
        "print(f\"Tuned batch size: {tuned_S}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code cell above, ``tune_batch_size_manual`` repeatedly samples ``test_replicates`` mean cost vectors, applies SciPy's **normaltest** along each dimension, prints the minimum p–value, and doubles ``S`` until all p–values exceed the significance threshold.  Once the condition is met, it returns the tuned batch size and the corresponding sample means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run the multivariate normal dueling bandit\n",
        "\n",
        "Now that we have a tuned batch size, we can run the dueling bandit.  The function `run_mvn_dueling_bandit` encapsulates the entire adaptive procedure: it reuses the normality tuning procedure internally (so we set ``initial_S=tuned_S`` to avoid starting from scratch), computes a multivariate normal prior, and updates the posterior as new floods are simulated.  The algorithm stops when the posterior probability that some height is optimal exceeds ``1 − delta``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "delta = 0.05  # target misselection probability\n",
        "rng = np.random.default_rng(98765)\n",
        "\n",
        "# Run the bandit algorithm with the tuned initial_S\n",
        "best_height, history = run_mvn_dueling_bandit(\n",
        "    heights=heights,\n",
        "    damage_costs=damage_costs,\n",
        "    protection_costs=protection_costs,\n",
        "    candidate_indices=candidate_indices,\n",
        "    years_all=years_future,\n",
        "    X_pred_paths_cm=X_future_paths,\n",
        "    posterior_params=posterior_params,\n",
        "    years_range=years_range,\n",
        "    delta=delta,\n",
        "    initial_S=tuned_S,    # use the manually tuned S as a starting point\n",
        "    significance=0.05,\n",
        "    test_replicates=200,\n",
        "    post_samples=2000,\n",
        "    max_rounds=50_000,\n",
        "    check_every=1_000,\n",
        "    rng=rng,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(f\"Selected best design height: {best_height:.2f} m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Inspect the convergence history\n",
        "\n",
        "The `history` dictionary returned by `run_mvn_dueling_bandit` contains the rounds at which the stopping rule was evaluated, the index and value of the current best height, and the posterior probability `p_best` that this candidate is optimal.  We can convert this to a Pandas DataFrame and plot the probability of the best height and the evolution of the selected height over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Build DataFrame\n",
        "hist_df = pd.DataFrame(history)\n",
        "display(hist_df.tail())\n",
        "\n",
        "# Plot posterior probability of the best height\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.plot(hist_df['rounds'], hist_df['p_best'], marker='o')\n",
        "ax1.axhline(1 - delta, color='red', linestyle='--', label=f'1 - delta = {1 - delta}')\n",
        "ax1.set_xlabel('Rounds')\n",
        "ax1.set_ylabel('Posterior prob. best')\n",
        "ax1.set_title('Convergence of the posterior probability for the best height')\n",
        "ax1.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot evolution of the empirical best height\n",
        "fig, ax2 = plt.subplots()\n",
        "ax2.step(hist_df['rounds'], hist_df['best_height'], where='post', marker='o')\n",
        "ax2.set_xlabel('Rounds')\n",
        "ax2.set_ylabel('Height (m)')\n",
        "ax2.set_title('Evolution of the empirically best height')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.13.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
